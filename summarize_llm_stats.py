# summarize_llm_stats.py â€” v7-ar (Arabic)
# Ù…Ù„Ø®Øµ ÙŠÙˆÙ…ÙŠ Ø´Ø§Ù…Ù„ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ù† Ù…ØµØ¯Ø± llm-stats (README) ÙˆÙŠØ±Ø³Ù„Ù‡ Ø¹Ù„Ù‰ ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù….
# - ÙŠØ¬Ù„Ø¨ Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù€ Leaderboard Ù…Ù† README ÙÙŠ Ù…Ø³ØªÙˆØ¯Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.
# - ÙŠØ³ØªØ®Ø±Ø¬ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙˆÙŠØ¬Ù…Ø¹ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ù…Ø²ÙˆÙ‘Ø¯.
# - ÙŠØ³ØªØ¨Ø¹Ø¯ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©/Ø§Ù„Ù…ÙØªÙˆØ­Ø© ÙˆÙÙ‚ EXCLUDE_REGEX (Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªØ¹Ø¯ÙŠÙ„).
# - ÙŠÙ‚Ø³Ù… Ø§Ù„Ø±Ø³Ø§Ù„Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¥Ø°Ø§ ØªØ¹Ø¯Ù‘Øª Ø­Ø¯ ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù….
#
# Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© ÙÙŠ Secrets:
#   TELEGRAM_BOT_TOKEN
#   TELEGRAM_CHAT_ID
# (Ø§Ø®ØªÙŠØ§Ø±ÙŠ) EXCLUDE_REGEX Ù„ØªØ®ØµÙŠØµ Ø§Ù„Ø§Ø³ØªØ¨Ø¹Ø§Ø¯.
#
import os, re, sys, requests

RAW_README = "https://raw.githubusercontent.com/JonathanChavezTamales/llm-leaderboard/main/README.md"

# Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©/Ø§Ù„Ù…ÙØªÙˆØ­Ø© (ØªÙ‚Ø¯Ø± ØªØºÙŠÙ‘Ø±Ù‡Ø§ Ù…Ù† Ø³ÙŠÙƒØ±ÙŠØª EXCLUDE_REGEX)
EXCLUDE_REGEX = os.getenv("EXCLUDE_REGEX", r"(?i)\b(llama|phi|gemma|mixtral|yi)\b")

# Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø© ÙÙŠ README
COL_ALIASES = {
    "name": ["name", "model", "model name"],
    "provider": ["provider", "company"],
    "humaneval": ["humaneval", "human eval", "aider polyglot", "code"],
    "aime2024": ["aime 2024", "aime-2024", "aime"],
    "gpqa": ["gpqa", "gpqa diamond", "knowledge"],
    "mmlu": ["mmlu"],
    "mmlupro": ["mmlu-pro", "mmlu pro"],
    "mmmu": ["mmmu", "multimodal"],
    "math": ["math", "gsm8k"],
    # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© (ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…Ø© ÙÙŠ Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø­Ø§Ù„ÙŠØ§Ù‹)
    "input_price": ["input price", "input $/1k tokens", "input $/1m"],
    "output_price": ["output price", "output $/1k tokens", "output $/1m"],
    "context": ["context", "context window", "ctx"],
}

# ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø²ÙˆÙ‘Ø¯ÙŠÙ† ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¹Ù†Ø¯ ØºÙŠØ§Ø¨ Ø¹Ù…ÙˆØ¯ Provider
PROVIDER_PATTERNS = [
    ("OpenAI", r"(?i)\b(gpt|o1|o3|o4)\b"),
    ("Anthropic", r"(?i)\b(claude)\b"),
    ("Google", r"(?i)\b(gemini|palm)\b"),
    ("xAI", r"(?i)\b(grok)\b"),
    ("Mistral", r"(?i)\b(mistral)\b"),
    ("DeepSeek", r"(?i)\b(deepseek)\b"),
    ("Alibaba/Qwen", r"(?i)\b(qwen)\b"),
    ("Kimi", r"(?i)\b(kimi)\b"),
    ("Cohere", r"(?i)\b(cohere|command)\b"),
    ("Perplexity", r"(?i)\b(perplexity|pplx|sonar)\b"),
    ("Other", r".*"),
]

# ØªØ³Ù…ÙŠØ§Øª Ø¹Ø±Ø¨ÙŠØ© Ù„Ù„ÙØ¦Ø§Øª
CAT_AR = {
    "Coding": "Ø§Ù„Ø¨Ø±Ù…Ø¬Ø©",
    "Math": "Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª",
    "Knowledge": "Ø§Ù„Ù…Ø¹Ø±ÙØ©",
    "Multimodal": "Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø·",
}

# ØªØ³Ù…ÙŠØ§Øª Ø¹Ø±Ø¨ÙŠØ© Ù„Ù„Ù…Ø²ÙˆÙ‘Ø¯ÙŠÙ†
PROVIDER_AR = {
    "OpenAI": "Ø£ÙˆØ¨Ù† Ø£ÙŠ Ø¢ÙŠ",
    "Anthropic": "Ø£Ù†Ø«Ø±ÙˆØ¨ÙŠÙƒ",
    "Google": "Ù‚ÙˆÙ‚Ù„",
    "xAI": "xAI",
    "Mistral": "Ù…ÙŠØ³ØªØ±Ø§Ù„",
    "DeepSeek": "Ø¯ÙŠØ¨ Ø³ÙŠÙƒ",
    "Alibaba/Qwen": "Ø¹Ù„ÙŠ Ø¨Ø§Ø¨Ø§ / ÙƒÙÙˆÙŠÙ†",
    "Kimi": "ÙƒÙŠÙ…ÙŠ",
    "Cohere": "ÙƒÙˆÙ‡ÙŠØ±",
    "Perplexity": "Ø¨ÙŠØ±Ø¨Ù„ÙŠÙƒØ³ÙŠØªÙŠ",
    "Other": "Ø£Ø®Ø±Ù‰",
}

def _to_float(text):
    if text is None:
        return float("nan")
    if isinstance(text, (int, float)):
        return float(text)
    s = str(text).strip().replace("%", "").replace(",", "")
    m = re.search(r"([0-9]+(?:\.[0-9]+)?)", s)
    if not m:
        return float("nan")
    try:
        return float(m.group(1))
    except:
        return float("nan")

def _norm(x):
    if x != x:
        return x
    return x/100.0 if 1 < x <= 100 else x

def fetch_readme():
    r = requests.get(RAW_README, timeout=45)
    if r.status_code != 200:
        raise RuntimeError(f"README fetch failed: {r.status_code}")
    return r.text

def parse_table(md):
    lines = [ln.rstrip() for ln in md.splitlines()]
    tables = []
    i = 0
    while i < len(lines)-1:
        if lines[i].strip().startswith("|") and "|" in lines[i] and set(lines[i+1].replace(" ", "")) >= set("|-:"):
            j = i
            buf = []
            while j < len(lines) and lines[j].strip().startswith("|"):
                buf.append(lines[j])
                j += 1
            tables.append(buf)
            i = j
        else:
            i += 1
    def header_cols(s):
        return [c.strip().lower() for c in s.split("|") if c.strip()]
    chosen = None
    for t in tables:
        header = header_cols(t[0])
        if any(k in header for k in ["name","model"]) and any(any(a in header for a in arr) for arr in COL_ALIASES.values()):
            chosen = t
            break
    if not chosen:
        return [], {}
    header = header_cols(chosen[0])
    col_idx = {}
    for key, aliases in COL_ALIASES.items():
        for a in aliases:
            if a in header:
                col_idx[key] = header.index(a)
                break
    rows = []
    for ln in chosen[2:]:
        if not ln.strip().startswith("|"):
            continue
        cols = [c.strip() for c in ln.split("|") if c.strip()]
        if len(cols) < 2:
            continue
        rows.append(cols)
    return rows, col_idx

def infer_provider(name, provider_col):
    if provider_col and provider_col.strip():
        return provider_col.strip()
    for label, pat in PROVIDER_PATTERNS:
        if re.search(pat, name):
            return label
    return "Other"

def build_models(rows, col_idx):
    models = []
    for cols in rows:
        name = cols[col_idx.get("name", 0)]
        if re.search(EXCLUDE_REGEX, name):
            continue
        provider_col = cols[col_idx.get("provider", -1)] if "provider" in col_idx else ""
        provider = infer_provider(name, provider_col)

        def g(key):
            if key not in col_idx: return float("nan")
            v = _to_float(cols[col_idx[key]])
            return _norm(v)

        scores = {
            "Coding": max(g("humaneval"), g("math")),  # Ø§Ù„Ø£ÙØ¶Ù„ÙŠØ© Ù„Ù€ HumanEval Ø¥Ù† ÙˆØ¬Ø¯
            "Math": max(g("aime2024"), g("math")),
            "Knowledge": max(g("gpqa"), g("mmlupro"), g("mmlu")),
            "Multimodal": g("mmmu"),
        }
        models.append({"name": name, "provider": provider, "scores": scores})
    return models

def group_by_provider(models):
    groups = {}
    for m in models:
        groups.setdefault(m["provider"], []).append(m)
    # ØªØ±ØªÙŠØ¨ Ø¯Ø§Ø®Ù„ ÙƒÙ„ Ù…Ø²ÙˆÙ‘Ø¯ Ø­Ø³Ø¨ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©
    def avg(m):
        vals = [v for v in m["scores"].values() if v == v]
        return sum(vals)/len(vals) if vals else 0.0
    for prov in groups:
        groups[prov] = sorted(groups[prov], key=avg, reverse=True)
    return groups

def fmt_pct(x): return f"{round(x*100,1)}%" if x==x else "â€”"

def build_messages(groups):
    order = ["OpenAI","Anthropic","Google","xAI","Mistral","DeepSeek",
             "Alibaba/Qwen","Kimi","Cohere","Perplexity","Other"]
    parts = []
    header = "ğŸ“Š Ù…Ù„Ø®Ù‘Øµ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ â€” ØªÙ‚Ø±ÙŠØ± ÙŠÙˆÙ…ÙŠ\n"
    curr = header
    for prov in order:
        if prov not in groups: continue
        prov_name = PROVIDER_AR.get(prov, prov)
        block = [f"â€” {prov_name}:"]
        for m in groups[prov]:
            s = m["scores"]
            line = (f"  â€¢ {m['name']}: "
                    f"{CAT_AR['Coding']} {fmt_pct(s['Coding'])}ØŒ "
                    f"{CAT_AR['Math']} {fmt_pct(s['Math'])}ØŒ "
                    f"{CAT_AR['Knowledge']} {fmt_pct(s['Knowledge'])}ØŒ "
                    f"{CAT_AR['Multimodal']} {fmt_pct(s['Multimodal'])}")
            block.append(line)
        block_text = "\n".join(block) + "\n\n"
        if len(curr) + len(block_text) > 3800:
            parts.append(curr.rstrip())
            curr = header + block_text
        else:
            curr += block_text
    parts.append(curr.rstrip())
    parts[-1] += "\nØ§Ù„Ù…ØµØ¯Ø±: llm-stats.com (Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† README)"
    return parts

def send_messages(msgs):
    token = os.getenv("TELEGRAM_BOT_TOKEN"); chat = os.getenv("TELEGRAM_CHAT_ID")
    if not token or not chat:
        print("Missing TELEGRAM_BOT_TOKEN or TELEGRAM_CHAT_ID.", file=sys.stderr); sys.exit(2)
    url = f"https://api.telegram.org/bot{token}/sendMessage"
    for m in msgs:
        r = requests.post(url, json={"chat_id": chat, "text": m, "parse_mode": "Markdown"}, timeout=60)
        if r.status_code != 200:
            raise RuntimeError(f"Telegram send failed: {r.status_code} {r.text}")

def main():
    md = fetch_readme()
    rows, col_idx = parse_table(md)
    if not rows:
        raise SystemExit("No leaderboard table parsed from README.")
    models = build_models(rows, col_idx)
    groups = group_by_provider(models)
    msgs = build_messages(groups)
    send_messages(msgs)
    print(f"Sent {len(msgs)} Telegram messages. Providers: {list(groups.keys())}")

if __name__ == "__main__":
    main()
